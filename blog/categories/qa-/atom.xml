<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: QA, | Tailored Shapes]]></title>
  <link href="http://tsmarsh.github.com/blog/categories/qa-/atom.xml" rel="self"/>
  <link href="http://tsmarsh.github.com/"/>
  <updated>2013-03-08T07:07:31-06:00</updated>
  <id>http://tsmarsh.github.com/</id>
  <author>
    <name><![CDATA[Tom Marsh]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Automated Testing]]></title>
    <link href="http://tsmarsh.github.com/blog/2012/10/12/automated-testing/"/>
    <updated>2012-10-12T19:01:00-05:00</updated>
    <id>http://tsmarsh.github.com/blog/2012/10/12/automated-testing</id>
    <content type="html"><![CDATA[<p>I was introduced to Unit Testing back in 2007 on my first agile gig. At that point
Unit tests were defined to me as 'tests that proved individual units of code'. Database connections were a smell, but not forbidden; mocks, stubs were actively encouraged. The closer we got to 100% coverage the better, but we did at least have the sense to not test the language, so no constructor or property testing.</p>

<p>Functional tests were then defined to me as 'tests the prove that individual units work together'.
This usually meant writing unit tests, but with spring providing you with real objects rather than mocks and it was generally accepted that a real database / service would be used. This was often an after thought, as we already had close to 100% unit coverage. There purpose was to catch bugs that only occurred in the interaction of objects that would otherwise have only been tested in isolation. The functional suite was a little light, predominantly because it was never really clear if functional tests were really just unit tests.</p>

<p>Integration tests were then described as 'proving that the whole thing works together', and were originally HTTPUnit tests, until we 'advanced' towards selenium. They were slow and unreliable. The build would break for environmental reasons so often that it was a surprise when they broke for real.</p>

<p>Despite this I was sold, at least I was sold on Unit testing. It just made my code better and gave me a way of thinking about the problem before I thought about the solution, but there were a few problems.</p>

<ul>
<li>Slow tests: Unit tests got slow fast. This boiled down to

<ul>
<li>Database calls: you can perform 100 unit tests in the time it takes to make one database call</li>
<li>Over mocking / stubbing: reflection is 10-100 times slower than a normal function call and as test code is only run once the most VMs won't optimize it away.</li>
</ul>
</li>
<li>Writing test code often takes a lot longer than writing the production code.</li>
<li>Over testing:

<ul>
<li>having to change tests in the unit, functional and integration tests</li>
<li>having to change five unit tests for one change.</li>
</ul>
</li>
</ul>


<p>Five years on and my styles have changed. This is how I see automated testing now.</p>

<!-- more -->


<p>Source control describes the 'who', 'when' and 'why'. This works better the smaller the commit. If you can link your commit to bug / story tracking software all the better. Blame should allow you figure out why a piece of code looks the way it does.</p>

<p><em>Functional tests describe the 'what'.</em></p>

<p>They are the first tests that you write, and describe the API. A measure of a good functional test is that it doesn't change when you re-factor your code. They should only change when you enhance your code by adding or removing functionality. Real objects should be used whenever possible, but calls to databases and services should be avoided. Mocking and stubbing of external libraries, services and databases is encouraged only if they make the test faster and more reliable. These tests should be really, really fast.</p>

<p>This concept is stolen directly from the work of Dan North and Behavioral Driven Design although I have more frequently seen that applied to integration tests rather than functional tests and that almost never works.</p>

<p><em>Unit tests describe the 'how'.</em></p>

<p>The public methods that you are testing via functional tests should be light, with the computation happening in well named protected methods that are called from the public method within reason. These methods need unit testing as they describe 'how' you are implementing your API. Expect to write many unit tests in an effort to get your functional tests to pass.</p>

<p>Unit tests have similar life cycles to the code they test. You should expect a great deal of churn as your code evolves. With each change requiring a change to a unit test. If you can change the code without affecting a unit test you should be asking yourself if you are missing a unit test, or if your unit test was really a functional test.</p>

<p>If you did no other automated testing than this, you would be in really good shape. Follow this plan and you will have a test suite that executes hundreds of tests per second and each of those tests can be run in parallel. You will also have a nicely documented piece of software, with each line of code's purpose explained by a mixture of source control, functional and unit testing.</p>

<p>But you still need to do manual testing. In fact, from this point on, the testing should be orchestrated by someone other than the developer on the story and you will always need manual testing.</p>

<p>Integration tests are where functional tests meet reality. You use a real database, you might even hit real services, provided that you have as much control over those services as you do your database. The application is poked in the same way a user would poke it.</p>

<p>Two things separate my views on integration tests from the integration of old.</p>

<p>Firstly, I believe Selenium is the tool of last resort. If you use functional and unit tests as I've described for both your server and client side logic you already have a pretty comprehensive test suite. All that is left to test is that you get the right data back for the right inputs when everything is wired together. That can be achieved with something as simple as HTTPUnit or even just poking controllers from your XUnit tool. You do loose the ability to click the button or check that a pop-up pops, but you get a test suite that is hundreds of times faster, significantly simpler to maintain and reliable.</p>

<p>Selenium is a miracle of modern engineering, built by some of the most talented and passionate developers in the world, but they are totally beholden to the teams that build browsers who, whilst selenium might be on their priority list, it is towards the bottom. The result is that browser automation tests crash. They crash because browsers don't like being poked by anything other than a human at human speeds. If you absolutely have to test that your pop-up pops every build, automatically, Selenium is the best tool, it just may be more expensive than you realized.</p>

<p>Secondly, I think that they need to be owned by some one who is directly responsible for efficiency of the team, not the functionality / efficiency of the application. Team / Tech / QA leads or the PM are all good candidates, business owners and the development team are not. There needs to be a budget for integration tests and that budget needs to be tracked by the project as a whole. If an integration tests breaks for a legitimate business reason developers first should inform QA that they need to manually test that part of the code now and then deactivate that test.</p>

<p>It is then the responsibility of whomever owns the integration build to get time allocated to fix it.</p>

<p>Making integration tests track-able outside of the normal story cost forces the owners to focus on keeping them as cheap and efficient as possible and stops the anti-pattern of having an 'acceptance' test for each story. The reason for this is that integration tests take minutes to run, and are rarely embarrassingly parallel. Adding minutes to the build has a direct effect on velocity and whilst integration tests can save a significant amount of QA churn, they are not a silver bullet. There needs to be evidence that the integration suite is actually improving velocity before each and every minute is added.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Desk Checking]]></title>
    <link href="http://tsmarsh.github.com/blog/2012/10/10/desk-checking/"/>
    <updated>2012-10-10T05:38:00-05:00</updated>
    <id>http://tsmarsh.github.com/blog/2012/10/10/desk-checking</id>
    <content type="html"><![CDATA[<p>If there is one practice guaranteed to reduce the number of bugs in a system it is desk checking.</p>

<p>The process should look something like this:</p>

<ol>
<li>Developer believes they have finished a story</li>
<li>Developer integrates with the latest version of the code</li>
<li>Developer runs automated tests</li>
<li>Developer asks the QA to 'Desk Check' their code</li>
<li>QA runs a quick, manual smoke test that puts the system through its paces</li>
<li>If any errors are found, Developer and QA discuss mechanisms for automating that testing at a unit or functional level.</li>
<li>Developer implements those tests, fixes problem and repeats.</li>
</ol>


<p>Up until step four this process should be familiar to all developers. In practice, steps four through six rarely take more than five minutes, so if it catches just a single bug it can save the project an hour of developer context switching. There are other subtle benefits as well. Letting developers see how you test can help reduce the time it takes to test even further, it may even help them to identify other problems.</p>

<p>All parties involved in defect resolution agree that early diagnosis and resolution of bugs is good for everyone, but 'Desk Checking' is often met with resistance from both QA and Developers.</p>

<p>I've experienced complaints from QA that desk checking interferes with normal testing, but really the attitude should be that 'normal testing' is the filler between desk checks, they are that effective.</p>

<p>It is also a little confrontational. Developers can react badly to having their weaknesses exposed so publicly and personally. This is just an unhealthy attitude on the part of the developer and should be considered a smell by their management. Remember, it may not be productive in the long run, but making a developer cry is a QA rite of passage.</p>
]]></content>
  </entry>
  
</feed>
